{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Face Recogniotion using SIFT Feature and cobining SVM result with X gboost \n",
    "detecting face gender Male or Femal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import needed library "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.11.0 64-bit' requires ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/hadil/AppData/Local/Programs/Python/Python311/python.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "import glob\n",
    "import xgboost as xgb\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1- Data Gathering \n",
    "Reading  the dataset Function that take the path of data set folder and read the image on gray level and sorted on Images list then encoded label to intgernuber and stor it on Labels list \n",
    "returm two listes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReadDataSet(path_of_dataSet):\n",
    "   Images_List=[]\n",
    "   labels_List=[]\n",
    "\n",
    "   # Reading ......\n",
    "   for directory_path in glob.glob(path_of_dataSet):\n",
    "       label = directory_path.split(\"\\\\\")[-1]\n",
    "       print(label)\n",
    "       for img_path in glob.glob(os.path.join(directory_path, \"*.jpg\")): #jpg\n",
    "           img = cv2.imread(img_path,0)# 0 for indicate the  gray scale   \n",
    "           Images_List.append(img)\n",
    "           labels_List.append(label)\n",
    "   # Label Encode      \n",
    "   le = preprocessing.LabelEncoder()\n",
    "   le.fit (labels_List)\n",
    "   Encoded_labels_List = le.transform(labels_List)\n",
    "   Images_List = np.array(Images_List , dtype='object')\n",
    "   Encoded_labels_List = np.array(Encoded_labels_List)\n",
    "   return Images_List , Encoded_labels_List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2- Featuer Extraction / Selection \n",
    "using the SIFT to extract the  interesting Keypoints and describtor for each Image (Extraction)\n",
    "the number of keypoint is theta that will be tunning to get the best value for this data set (Selection)\n",
    "then return the describtors list for each images as feature (Feature set)\n",
    "Refrences : https://github.com/Akhilesh64/Image-Classification-using-SIFT/blob/main/main.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SIFT(Images_List, thresh):\n",
    "\n",
    "  t0 = time.time()\n",
    "\n",
    "\n",
    "  def CalcFeatures(img, th):\n",
    "    sift = cv2.xfeatures2d.SIFT_create(th)\n",
    "    kp, des = sift.detectAndCompute(img, None)\n",
    "    return des\n",
    "  \n",
    "  '''\n",
    "  All  Images list are passed through the CalcFeatures functions\n",
    "   which returns the descriptors which are appended to the features list and \n",
    "  then stacked vertically in the form of a numpy array.\n",
    "  '''\n",
    "\n",
    "  features = []\n",
    "  for img in Images_List:\n",
    "    img_des = CalcFeatures(img, thresh)\n",
    "    if img_des is not None:\n",
    "      features.append(img_des)\n",
    "  features = np.vstack(features)\n",
    "\n",
    "  '''\n",
    "  K-Means clustering is then performed on the feature array obtained \n",
    "  from the previous step. The centres obtained after clustering are \n",
    "  further used for bagging of features.\n",
    "  '''\n",
    "\n",
    "  k = 150\n",
    "  criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 10, 0.1)\n",
    "  flags = cv2.KMEANS_RANDOM_CENTERS\n",
    "  compactness, labels, centres = cv2.kmeans(features, k, None, criteria, 10, flags)\n",
    "\n",
    "  '''\n",
    "  The bag_of_features function assigns the features which are similar\n",
    "  to a specific cluster centre thus forming a Bag of Words approach.  \n",
    "  '''\n",
    "\n",
    "  def bag_of_features(features, centres, k = 500):\n",
    "      vec = np.zeros((1, k))\n",
    "      for i in range(features.shape[0]):\n",
    "          feat = features[i]\n",
    "          diff = np.tile(feat, (k, 1)) - centres\n",
    "          dist = pow(((pow(diff, 2)).sum(axis = 1)), 0.5)\n",
    "          idx_dist = dist.argsort()\n",
    "          idx = idx_dist[0]\n",
    "          vec[0][idx] += 1\n",
    "      return vec\n",
    "\n",
    "  vec = []\n",
    "  for img in Images_List:\n",
    "    img_des = CalcFeatures(img, thresh)\n",
    "    if img_des is not None:\n",
    "      img_vec = bag_of_features(img_des, centres, k)\n",
    "      vec.append(img_vec)\n",
    "  vec = np.vstack(vec)\n",
    "\n",
    "  t1 = time.time()\n",
    "  \n",
    "  return vec, (t1-t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3- Classification \n",
    "on classification there will be two classifier used on this step Support Vector Machine (SVM) and Xgboost \n",
    "- SVM the parameter used ( kernal and gamma)\n",
    "poly keranl have been choicen because have return better accureacy the degree will be the default(The Best)\n",
    "gamma will be 0.6 it's the best after tuning\n",
    "performance measurment calaculated are  Accurecy , Confusion matrix AUC and Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVM_Classifier(DataSet,Label_List):\n",
    "     # Split to Train and Test\n",
    "     X_train, X_test, y_train, y_test = train_test_split(DataSet,Label_List,test_size=.2)\n",
    "                             \n",
    "     # define support vector classifier\n",
    "     svm = SVC(kernel='poly',gamma = 0.6) #probability=True, \n",
    "     svm.fit(X_train,y_train)\n",
    "     y_pred = svm.predict(X_test)\n",
    "\n",
    "     # calculate accuracy, specificity, sensitive and F1\n",
    "     accuracy = accuracy_score(y_test, y_pred)\n",
    "    #  specificity = recall_score(y_test, y_pred,average= None)\n",
    "    #  sensitive = recall_score(np.logical_not(y_test) , np.logical_not(y_pred),average= None)\n",
    "    #  F1 = f1_score(y_test, y_pred, average= None)\n",
    "    #  report = classification_report(y_test, y_pred,zero_division =1, target_names=['Female', 'Male'])#,'Fear', 'Happy','Neutral','Sad','Suprise'\n",
    "     conf_mat = confusion_matrix(y_test, y_pred)\n",
    "     \n",
    "    #  # Calculate AUC\n",
    "    #  probabilities = svm.predict_proba(X_test)\n",
    "    #  y_proba = probabilities[:, 1]\n",
    "    #  # calculate false positive rate and true positive rate at different thresholds\n",
    "    #  false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_proba, pos_label=1)\n",
    "    #  AUC = auc(false_positive_rate, true_positive_rate)\n",
    "     \n",
    "\n",
    "     return accuracy*100,conf_mat #specificity,sensitive,F1,report,false_positive_rate,true_positive_rate,AUC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- XgBoost\n",
    "performance measurment calaculated are  Accurecy , Confusion matrix AUC and Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def XgBoost_Classifier(DataSet,Label_List):\n",
    "    # Split to Train and Test\n",
    "     X_train, X_test, y_train, y_test = train_test_split(DataSet,Label_List,test_size=.2)\n",
    "\n",
    "     model = xgb.XGBClassifier()\n",
    "     model.fit(X_train,y_train)\n",
    "     y_pred = model.predict(X_test)\n",
    "     # Caculate accuracy,specificity,sensitive,F1,report\n",
    "     accuracy= accuracy_score( y_pred,y_test)\n",
    "     conf_mat = confusion_matrix(y_test, y_pred)\n",
    "    #  specificity = recall_score(y_test, y_pred,average= None)\n",
    "    #  sensitive = recall_score(np.logical_not(y_test) , np.logical_not(y_pred),average= None)\n",
    "    #  F1 = f1_score(y_test, y_pred, average= None)\n",
    "    #  report = classification_report(y_test, y_pred,zero_division =1, target_names=['Femal','male'])\n",
    "     \n",
    "    #  #AUC\n",
    "    #  probabilities =model.predict_proba(X_test)\n",
    "    #  y_proba = probabilities[:, 1]\n",
    "    #  # calculate false positive rate and true positive rate at different thresholds\n",
    "    #  false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_proba, pos_label=1)\n",
    "    #  AUC = auc(false_positive_rate, true_positive_rate)\n",
    "    \n",
    "     return accuracy*100,conf_mat #specificity,sensitive,F1,report ,false_positive_rate,true_positive_rate, AUC\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main\n",
    "- Reading data set from folder and get the Images and label list \n",
    "- looping throgth different value of threshold \n",
    "- getting the featuer set then send it to classifer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image_List,Label_list =ReadDataSet(r\"Subset/*\")\n",
    "\n",
    "accuracy = []\n",
    "timer = []\n",
    "threshold = [] \n",
    "for i in range(5,26,5): #SVM  15,26,5 \n",
    "  print('\\nCalculating for a threshold of {}'.format(i))\n",
    "  feature, time_consuming  = SIFT(Image_List,i)\n",
    "  data= SVM_Classifier(feature, Label_list)\n",
    "  xgboostdata= SVM_Classifier(feature, Label_list)\n",
    "  accuracy.append(data[0])\n",
    "  conf_mat = data[1]\n",
    "  threshold.append(i)\n",
    "  timer.append(time_consuming)\n",
    "  print('\\ngamma= {}\\nAccuracy = {}\\nTime taken = {} sec\\nConfusion matrix :\\n{}'.format(data[0],time_consuming,data[1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plooting AUC\n",
    "\n",
    "correct accurecy "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a41862677209d0f6c6f72b3b118395bce78e978d9d3b978a0de6483880a01472"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
